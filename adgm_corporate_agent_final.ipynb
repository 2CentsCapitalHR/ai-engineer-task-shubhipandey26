{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Dependencies**"
      ],
      "metadata": {
        "id": "POrxYygJ3vTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfO8YVYU19HP",
        "outputId": "30083d2c-d24a-4bbc-eda9-9df3e72e8ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet python-docx gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports + Create Folders + Default Rules + Sample docx**"
      ],
      "metadata": {
        "id": "C7xDs2dW34J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, zipfile, re, textwrap\n",
        "from docx import Document\n",
        "import gradio as gr\n",
        "\n",
        "# Create folders\n",
        "os.makedirs('rules', exist_ok=True)\n",
        "os.makedirs('rules_texts', exist_ok=True)   # optional ADGM text files go here\n",
        "os.makedirs('examples', exist_ok=True)\n",
        "os.makedirs('output', exist_ok=True)\n",
        "\n",
        "# Default (example) ADGM rules JSON.\n",
        "# You should replace/add entries in rules/adgm_rules.json with the actual checklists and snippets\n",
        "sample_rules = {\n",
        "  \"company_incorporation\": {\n",
        "    \"display_name\": \"Company Incorporation\",\n",
        "    \"required_docs\": [\n",
        "      \"Articles of Association\",\n",
        "      \"Memorandum of Association\",\n",
        "      \"UBO Declaration Form\",\n",
        "      \"Incorporation Application Form\",\n",
        "      \"Register of Members and Directors\"\n",
        "    ],\n",
        "    # keywords used to detect doc types inside uploaded .docx\n",
        "    \"doc_type_keywords\": {\n",
        "      \"Articles of Association\": [\"articles of association\", \"aoa\"],\n",
        "      \"Memorandum of Association\": [\"memorandum of association\", \"moa\", \"memorandum\"],\n",
        "      \"UBO Declaration Form\": [\"ubo declaration\", \"ultimate beneficial owner\"],\n",
        "      \"Incorporation Application Form\": [\"incorporation application\", \"incorporation application form\"],\n",
        "      \"Register of Members and Directors\": [\"register of members\", \"register of directors\"]\n",
        "    },\n",
        "    # red flag rules: pattern -> if must_exist==True then it's required (issue if missing),\n",
        "    # otherwise issue if the pattern is found.\n",
        "    \"red_flags\": [\n",
        "      {\"pattern\": \"UAE Federal Courts\", \"label\": \"Incorrect jurisdiction\", \"must_exist\": False, \"severity\": \"High\", \"suggestion\": \"Replace with 'ADGM Courts'.\"},\n",
        "      {\"pattern\": \"signature\", \"label\": \"Missing signature block\", \"must_exist\": True, \"severity\": \"High\", \"suggestion\": \"Add a signature block and dated signature.\"},\n",
        "      {\"pattern\": \"ambiguous\", \"label\": \"Ambiguous language\", \"must_exist\": False, \"severity\": \"Medium\", \"suggestion\": \"Clarify the clause to remove ambiguous terms.\"}\n",
        "    ],\n",
        "    # optional static citation snippets to include in comments when relevant\n",
        "    \"citation_snippets\": {\n",
        "      \"Incorrect jurisdiction\": \"Per ADGM Companies Regulations 2020, jurisdiction must reference ADGM Courts (example snippet).\",\n",
        "      \"Missing signature block\": \"ADGM requires signature blocks for executed documents per registration guidance (example snippet).\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "rules_path = 'rules/adgm_rules.json'\n",
        "if not os.path.exists(rules_path):\n",
        "    with open(rules_path,'w') as f:\n",
        "        json.dump(sample_rules, f, indent=2)\n",
        "    print(f\"Created example rules at {rules_path} (edit this with ADGM content if you have it).\")\n",
        "else:\n",
        "    print(f\"Using existing rules at {rules_path}\")\n",
        "\n",
        "# create a small sample .docx (so you can test immediately)\n",
        "sample_doc = 'examples/sample_before.docx'\n",
        "if not os.path.exists(sample_doc):\n",
        "    doc = Document()\n",
        "    doc.add_heading('Articles of Association', level=1)\n",
        "    doc.add_paragraph('This is the Articles of Association for the company.')\n",
        "    doc.add_paragraph('Jurisdiction: UAE Federal Courts')  # intentionally bad to trigger red flag\n",
        "    doc.add_paragraph('Some unclear ambiguous clause that needs clarity.')\n",
        "    # note: intentionally leave signature blank to trigger missing-signature detection\n",
        "    doc.add_paragraph('Date: ')\n",
        "    doc.save(sample_doc)\n",
        "    print(f\"Sample doc created at {sample_doc}\")\n",
        "else:\n",
        "    print(f\"Sample doc already exists at {sample_doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmbSQp6W1_Ci",
        "outputId": "55db02da-7168-4239-b082-c2c06b69323e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created example rules at rules/adgm_rules.json (edit this with ADGM content if you have it).\n",
            "Sample doc created at examples/sample_before.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper Functions**\n",
        "**(extraction, detection, comment insertion, local retrieval)**"
      ],
      "metadata": {
        "id": "aXX-ku8U4Aqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from pathlib import Path\n",
        "\n",
        "# Load rules\n",
        "with open('rules/adgm_rules.json','r') as f:\n",
        "    adgm_rules = json.load(f)\n",
        "\n",
        "def extract_paragraphs(docx_path):\n",
        "    \"\"\"Return a list of non-empty paragraph texts from a .docx file.\"\"\"\n",
        "    doc = Document(docx_path)\n",
        "    paras = [p.text for p in doc.paragraphs]\n",
        "    # keep even blank paragraphs for indexing consistency — but strip empties if desired\n",
        "    return paras\n",
        "\n",
        "def text_of_doc(docx_path):\n",
        "    \"\"\"Return full text as single string (joined paragraphs).\"\"\"\n",
        "    paras = extract_paragraphs(docx_path)\n",
        "    return \"\\n\".join(paras)\n",
        "\n",
        "def detect_doc_types_in_text(text, rules):\n",
        "    \"\"\"\n",
        "    Given the text and a process rules dict, return list of matched doc types\n",
        "    using the doc_type_keywords mapping.\n",
        "    \"\"\"\n",
        "    found = []\n",
        "    ltext = text.lower()\n",
        "    for doc_name, keywords in rules.get(\"doc_type_keywords\", {}).items():\n",
        "        for kw in keywords:\n",
        "            if kw.lower() in ltext:\n",
        "                found.append(doc_name)\n",
        "                break\n",
        "    return found\n",
        "\n",
        "def detect_red_flags_in_text(text, rules):\n",
        "    \"\"\"\n",
        "    Returns list of issue dicts found in the text per rules['red_flags'].\n",
        "    Each issue has: label, pattern, severity, suggestion, location (paragraph idx or 'Not found'), snippet.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    paras = extract_paragraphs_from_text = text.splitlines()\n",
        "    ltext = text.lower()\n",
        "    for rf in rules.get(\"red_flags\", []):\n",
        "        pattern = rf[\"pattern\"].lower()\n",
        "        must_exist = rf.get(\"must_exist\", False)\n",
        "        label = rf.get(\"label\", pattern)\n",
        "        severity = rf.get(\"severity\", \"Medium\")\n",
        "        suggestion = rf.get(\"suggestion\", \"\")\n",
        "        # Case 1: pattern must exist but is missing -> issue\n",
        "        if must_exist:\n",
        "            if pattern not in ltext:\n",
        "                issues.append({\n",
        "                    \"label\": label,\n",
        "                    \"pattern\": rf[\"pattern\"],\n",
        "                    \"severity\": severity,\n",
        "                    \"suggestion\": suggestion,\n",
        "                    \"location\": \"Not found (required content missing)\",\n",
        "                    \"snippet\": \"\"\n",
        "                })\n",
        "            # if it exists, no issue\n",
        "            continue\n",
        "        # Case 2: pattern is problematic if present\n",
        "        if pattern in ltext:\n",
        "            # find first paragraph that contains the pattern (for contextual location)\n",
        "            location = None\n",
        "            snippet = \"\"\n",
        "            for idx, p in enumerate(paras):\n",
        "                if pattern in p.lower():\n",
        "                    snippet = p.strip()\n",
        "                    location = f\"Paragraph {idx+1}\"\n",
        "                    break\n",
        "            if location is None:\n",
        "                location = \"Found in document (no paragraph match)\"\n",
        "            issues.append({\n",
        "                \"label\": label,\n",
        "                \"pattern\": rf[\"pattern\"],\n",
        "                \"severity\": severity,\n",
        "                \"suggestion\": suggestion,\n",
        "                \"location\": location,\n",
        "                \"snippet\": snippet\n",
        "            })\n",
        "    return issues\n",
        "\n",
        "def local_retrieve_snippets(pattern, texts_folder='rules_texts', radius=120):\n",
        "    \"\"\"\n",
        "    Simulated local 'retrieval' from uploaded ADGM reference text files.\n",
        "    Returns a list of (filename, snippet) for files that contain the pattern.\n",
        "    If no files are present or no matches, returns [].\n",
        "    \"\"\"\n",
        "    matches = []\n",
        "    if not os.path.isdir(texts_folder):\n",
        "        return matches\n",
        "    for fname in os.listdir(texts_folder):\n",
        "        path = os.path.join(texts_folder, fname)\n",
        "        try:\n",
        "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read().lower()\n",
        "        except Exception:\n",
        "            continue\n",
        "        if pattern.lower() in content:\n",
        "            idx = content.index(pattern.lower())\n",
        "            start = max(0, idx - radius)\n",
        "            end = min(len(content), idx + len(pattern) + radius)\n",
        "            snippet = content[start:end].strip()\n",
        "            matches.append((fname, snippet))\n",
        "    return matches\n",
        "\n",
        "def append_review_comments(original_docx_path, issues, out_path, rules_for_process=None):\n",
        "    \"\"\"\n",
        "    Creates a reviewed copy of the original docx and appends\n",
        "    clear REVIEW COMMENT paragraphs that reference paragraph indexes or missing items.\n",
        "    \"\"\"\n",
        "    doc = Document(original_docx_path)\n",
        "    doc.add_paragraph(\"\")  # separator\n",
        "    doc.add_paragraph(\"----- REVIEWER COMMENTS (automated) -----\")\n",
        "    if not issues:\n",
        "        doc.add_paragraph(\"No issues detected by the offline checker.\")\n",
        "    else:\n",
        "        for i, issue in enumerate(issues, start=1):\n",
        "            header = f\"COMMENT {i}: [{issue.get('label')}] (severity: {issue.get('severity')})\"\n",
        "            doc.add_paragraph(header)\n",
        "            # location & snippet\n",
        "            doc.add_paragraph(f\"Location: {issue.get('location')}\")\n",
        "            snippet = issue.get('snippet', '')\n",
        "            if snippet:\n",
        "                # keep snippet reasonably short\n",
        "                doc.add_paragraph(\"Context snippet: \" + (snippet if len(snippet) < 500 else snippet[:500] + \"...\"))\n",
        "            # suggestion\n",
        "            doc.add_paragraph(\"Suggestion: \" + issue.get('suggestion','No suggestion available.'))\n",
        "            # try to add citation from rules if available\n",
        "            if rules_for_process and rules_for_process.get('citation_snippets'):\n",
        "                citation = rules_for_process['citation_snippets'].get(issue.get('label'))\n",
        "                if citation:\n",
        "                    doc.add_paragraph(\"ADGM reference (local rules file): \" + citation)\n",
        "            # as a fallback attempt to find a matching ADGM snippet in rules_texts\n",
        "            retrieved = local_retrieve_snippets(issue.get('pattern',''))\n",
        "            if retrieved:\n",
        "                doc.add_paragraph(\"Local ADGM snippet(s) from rules_texts:\")\n",
        "                for fname, snip in retrieved[:2]:\n",
        "                    doc.add_paragraph(f\" - {fname}: {snip[:400]}...\")\n",
        "            doc.add_paragraph(\"\")  # blank line between comments\n",
        "    doc.save(out_path)\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "pVqX12fo2Eba"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Pipeline**\n",
        "**(process multiple uploaded .docx files → produce reviewed docs + JSON summary + ZIP)**"
      ],
      "metadata": {
        "id": "U-JjpNTO4HCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_uploaded_files(uploaded_files):\n",
        "    \"\"\"\n",
        "    uploaded_files: single file or list of files uploaded via Gradio.\n",
        "    Returns (summary_dict, zip_path)\n",
        "    \"\"\"\n",
        "    # normalize input\n",
        "    if uploaded_files is None:\n",
        "        return {\"error\": \"No files provided\"}, None\n",
        "    if not isinstance(uploaded_files, (list, tuple)):\n",
        "        uploaded_files = [uploaded_files]\n",
        "    processed = []\n",
        "    # load rules once\n",
        "    with open('rules/adgm_rules.json','r') as f:\n",
        "        adgm_rules = json.load(f)\n",
        "    # We'll record all detected doc types across files to decide the process\n",
        "    for f in uploaded_files:\n",
        "        # Gradio's uploaded file objects usually have a .name attribute with the temp path\n",
        "        file_path = getattr(f, \"name\", None) or f\n",
        "        if not str(file_path).lower().endswith('.docx'):\n",
        "            # skip non-docx files\n",
        "            continue\n",
        "        # read text\n",
        "        text = text_of_doc(file_path)\n",
        "        # detect types & issues across all known processes\n",
        "        detected_types = []\n",
        "        issues_for_file = []\n",
        "        for process_key, rules in adgm_rules.items():\n",
        "            types = detect_doc_types_in_text(text, rules)\n",
        "            if types:\n",
        "                detected_types.extend(types)\n",
        "            issues = detect_red_flags_in_text(text, rules)\n",
        "            # attach process info to each issue\n",
        "            for iss in issues:\n",
        "                iss['detected_in_file'] = os.path.basename(file_path)\n",
        "                iss['process_key'] = process_key\n",
        "            issues_for_file.extend(issues)\n",
        "        detected_types = list(sorted(set(detected_types)))\n",
        "        reviewed_name = os.path.basename(file_path).replace('.docx','_reviewed.docx')\n",
        "        reviewed_path = os.path.join('output', reviewed_name)\n",
        "        # for creating comments include rules for best-guess process if exactly one process had matches,\n",
        "        # otherwise pass a generic empty rules dict\n",
        "        # (we will pick the final process below from aggregated counts)\n",
        "        tentative_process = None\n",
        "        # Choose tentative process as the one that has most doc keywords present in this file\n",
        "        best_score = 0\n",
        "        for pk, rules in adgm_rules.items():\n",
        "            score = 0\n",
        "            for dt in detected_types:\n",
        "                if dt in rules.get('required_docs', []):\n",
        "                    score += 1\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                tentative_process = pk\n",
        "        rules_for_comments = adgm_rules.get(tentative_process) if tentative_process else None\n",
        "        append_review_comments(file_path, issues_for_file, reviewed_path, rules_for_comments)\n",
        "        processed.append({\n",
        "            \"original_path\": file_path,\n",
        "            \"reviewed_path\": reviewed_path,\n",
        "            \"detected_types\": detected_types,\n",
        "            \"issues\": issues_for_file\n",
        "        })\n",
        "    # decide overall process: count matched required docs per process across all processed files\n",
        "    process_scores = {}\n",
        "    for pk, rules in adgm_rules.items():\n",
        "        required = set(rules.get('required_docs', []))\n",
        "        # how many of required docs were detected in uploaded files\n",
        "        detected_across = set()\n",
        "        for p in processed:\n",
        "            for d in p['detected_types']:\n",
        "                if d in required:\n",
        "                    detected_across.add(d)\n",
        "        process_scores[pk] = len(detected_across)\n",
        "    # pick process with highest score (or Unknown)\n",
        "    best_proc = max(process_scores.items(), key=lambda x: x[1])\n",
        "    if best_proc[1] == 0:\n",
        "        chosen_process = \"Unknown\"\n",
        "        required_documents = []\n",
        "        missing_documents = []\n",
        "    else:\n",
        "        chosen_process = best_proc[0]\n",
        "        required_documents = adgm_rules[chosen_process]['required_docs']\n",
        "        detected_across = set()\n",
        "        for p in processed:\n",
        "            for d in p['detected_types']:\n",
        "                detected_across.add(d)\n",
        "        missing_documents = [r for r in required_documents if r not in detected_across]\n",
        "    # prepare final JSON\n",
        "    issues_found = []\n",
        "    for p in processed:\n",
        "        for iss in p['issues']:\n",
        "            issues_found.append({\n",
        "                \"document\": os.path.basename(iss.get('detected_in_file', 'Unknown')),\n",
        "                \"label\": iss.get('label'),\n",
        "                \"section\": iss.get('location'),\n",
        "                \"issue\": f\"Matched pattern: {iss.get('pattern')}\",\n",
        "                \"severity\": iss.get('severity'),\n",
        "                \"suggestion\": iss.get('suggestion')\n",
        "            })\n",
        "    summary = {\n",
        "        \"process\": adgm_rules.get(chosen_process, {}).get('display_name', chosen_process) if chosen_process!=\"Unknown\" else \"Unknown\",\n",
        "        \"documents_uploaded\": len(processed),\n",
        "        \"required_documents\": len(required_documents),\n",
        "        \"missing_documents\": missing_documents,\n",
        "        \"issues_found\": issues_found\n",
        "    }\n",
        "    # create ZIP with all reviewed docs + summary.json\n",
        "    zip_path = os.path.join('output', 'reviewed_bundle.zip')\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "        # add reviewed docs\n",
        "        for p in processed:\n",
        "            zf.write(p['reviewed_path'], arcname=os.path.basename(p['reviewed_path']))\n",
        "        # add summary.json\n",
        "        summary_path = os.path.join('output','summary.json')\n",
        "        with open(summary_path,'w') as sf:\n",
        "            json.dump(summary, sf, indent=2)\n",
        "        zf.write(summary_path, arcname='summary.json')\n",
        "    return summary, zip_path"
      ],
      "metadata": {
        "id": "5QfHy5KT2FSp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio UI**"
      ],
      "metadata": {
        "id": "NZt5TQk54duj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_process(files):\n",
        "    summary, zip_path = process_uploaded_files(files)\n",
        "    return json.dumps(summary, indent=2), zip_path\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_process,\n",
        "    inputs=gr.File(file_count=\"multiple\", label=\"Upload one or more .docx files\"),\n",
        "    outputs=[gr.Textbox(label=\"Analysis JSON\"), gr.File(label=\"Download results (ZIP)\")],\n",
        "    title=\"ADGM Corporate Agent by Shubhi\",\n",
        "    description=\"Upload .docx files. The agent runs with keyword-based checks, appends reviewer comments into a reviewed .docx, and returns a ZIP with reviewed files + summary.json.\"\n",
        ")\n",
        "\n",
        "# In Colab, using share=True makes the UI reachable; you can set share=False but Colab often needs share=True to expose the UI.\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "XdDj4z-v2LbG",
        "outputId": "2c752564-91db-4df0-f05b-57713e3d54f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://20b46cc72c1763ce9a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://20b46cc72c1763ce9a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}